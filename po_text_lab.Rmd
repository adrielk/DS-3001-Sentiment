---
title: "text_lab"
date: "10/19/2021"
author: "Po Wei Tsao"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("ggwordcloud")
library(ggwordcloud)
#install.packages("gutenbergr") 
library(gutenbergr)
#install.packages('textdata')
library(textdata)
# setwd("/cloud/project/tidytext")
# save.image("tidytext.RData")
```

```{r}
doc1 <- read_lines("text_data/new_york_times/new_york_times_1.txt")
doc2 <- read_lines("text_data/new_york_times/new_york_times_2.txt")
doc3 <- read_lines("text_data/new_york_times/new_york_times_3.txt")
doc4 <- read_lines("text_data/new_york_times/new_york_times_4.txt")
doc5 <- read_lines("text_data/new_york_times/new_york_times_5.txt")
nyc_docs = paste(doc1, doc2, doc3, doc4, doc5, sep=" ")
# nyc_docs
nyc_docs <- tibble(nyc_docs)

nyc_docs$nyc_docs <- as.character(nyc_docs$nyc_docs)

nyc_docs <- nyc_docs %>%
  unnest_tokens(word, nyc_docs)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

View(nyc_docs)

```

```{r}
#Code that performs sentiment analysis

get_sentiments('afinn')# we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc')# looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing')# looks like a good amount more 6,776, but as we can see just negative and positive. 

nyc_docs_sentiment_afinn <- nyc_docs %>%
  inner_join(get_sentiments("afinn"))
  
nyc_docs_sentiment_nrc <- nyc_docs %>%
  inner_join(get_sentiments("nrc"))

nyc_docs_sentiment_bing <- nyc_docs %>%
  inner_join(get_sentiments("bing"))

# View(nyc_docs_sentiment_nrc)
# View(nyc_docs_sentiment_afinn)
# View(nyc_docs_sentiment_bing)

ggplot(data = nyc_docs_sentiment_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("New York Times Sentiment Range")+
  theme_minimal()

ggplot(data = nyc_docs_sentiment_nrc, 
       aes(x=sentiment),
        )+
  geom_histogram(stat = "count")+
  ggtitle("New York Times Sentiment Categories")+
  theme_minimal()

ggplot(data = nyc_docs_sentiment_bing, 
       aes(x=sentiment),
        )+
  geom_histogram(stat = "count")+
  ggtitle("New York Times Sentiment Positivity")+
  theme_minimal()

```

```{r}
# Code for the term frequency stuff
data_prep <- function(x){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",remove = TRUE,sep = "")
}

doc1 <- read_lines("text_data/new_york_times/new_york_times_1.txt")
doc2 <- read_lines("text_data/new_york_times/new_york_times_2.txt")
doc3 <- read_lines("text_data/new_york_times/new_york_times_3.txt")
doc4 <- read_lines("text_data/new_york_times/new_york_times_4.txt")
doc5 <- read_lines("text_data/new_york_times/new_york_times_5.txt")

doc1_bag <- data_prep(doc1)
# str(doc1_bag)

doc2_bag <- data_prep(doc2)
# str(doc2_bag) 

doc3_bag <- data_prep(doc3)
# str(doc1_bag)

doc4_bag <- data_prep(doc4)
# str(doc1_bag)

doc5_bag <- data_prep(doc5)
# str(doc1_bag)

docs <- c("doc1","doc2", "doc3", "doc4", "doc5")
tf_idf_text_ny <- tibble(docs,text=t(tibble(doc1_bag, doc2_bag, doc3_bag, doc4_bag, doc5_bag, .name_repair = "universal")))


word_count <- tf_idf_text_ny %>%
  unnest_tokens(word, text) %>%
  count(docs, word, sort = TRUE)

View(tf_idf_text_ny)

ny_total_words <- word_count %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))
ny_total_words

ny_doc_words <- left_join(word_count, ny_total_words)

ny_doc_words <- ny_doc_words %>%
  bind_tf_idf(word, docs, n)

ny_doc_words <- ny_doc_words %>%
  anti_join(stop_words)

View(ny_doc_words)

```
Conclusion:

As shown in the diagrams above, the New York Times has a clear negative bias in terms of its sentiment towards the topic of climate change. The methods I used for analyzing sentiment was to combine the articles into one string before analysis. This allows me to observe the entire newspaper as a whole because the data from the 5 articles I used are combined. After combining the source material, I used dplyr to remove stop words and unnecessary tokens. This cleaned the text of various symbols and meaningless words. From there, I utilized libraries (afinn, nrc and bing) that gave me data regarding the text that was mined from the articles. From these results, I generated 3 graphs (one for each library), allow for the biases of the New York Times to be clearly shown in a visual manner.

The results of the these visualizations are clear: the New York Times has a negative bias towards the topic of climate change. As one can see from the graph labeled "New York Times Sentiment Positivity", a majority of the words throughout the articles are negatively bias. Digging deeper into this idea, we can look at the graph titled "New York Times Sentiment Categories." While this graph shows that the "negative" bar is lower than the "positive" bar, we can take note that most of the other categories are biased towards negativity. Bars for the categories "fear", "disgust" and "anger" are a few of the more prominent ones, allowing us to further analyze and break down the negativity within the articles. Lastly, looking at the "New York Times Sentiment Range" graph, we can see that there are many words ranged in the -2 range. This shows us where exactly this negative bias falls on in a scale, telling us that the negative bias is definitely prominent. However, given that the number of words in the -3 category is rather small, we can conclude that the articles are not necessarily "extremely negative." Rather, they show the issue of climate change with a moderate negative bias.

In terms of the term frequencies and inverse document frequencies, words such as "climate" and "change" are frequent (as expected). Looking further down the list, we do see some interesting key words. For example, "warming", "flooding" and "weather" are among the many words that rank high in term frequency. This aligns with the negative bias we saw in the graphs earlier. The patterns of negativity are prominent, as demonstrated from the graphs and the term frequencies of "doomsday" words, such as "warming" and "flooding." The inverse document frequency also support that words such as "warming", "flooding" and "weather" are important key words within the corpus.

I think this negativity is because of many reasons. First of all, the New York Times is a very global newspaper, so its views would reflect the more popular beliefs of climate change being a looming threat to the world. This regions is also a more Democratic region, which aligns with the view that climate change is a threat and must be dealt with. To the non-profit, I would suggest that this region indicates many people are aware of the threats of climate change and see it as a very real threat. If it were to start campaigns to raise awareness about threats of climate change in that area, I believe it would garner lots of support.


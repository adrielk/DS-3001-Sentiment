---
title: "virginia_text_mining"
output: html_document
---

To Do: word map, recommendations (final report), Words ordered by frequency

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(tidyverse)
install.packages("tidytext")
library(tidytext)
install.packages("ggwordcloud")
library(ggwordcloud)
install.packages("gutenbergr") 
library(gutenbergr)
install.packages('textdata')
library(textdata)
```
```{r}

doc1 = read_lines("text_data/virginia/virginia_1.txt")
doc2 = read_lines("text_data/virginia/virginia_2.txt")
doc3 = read_lines("text_data/virginia/virginia_3.txt")
doc4 = read_lines("text_data/virginia/virginia_4.txt")
doc5 = read_lines("text_data/virginia/virginia_5.txt")


doc_full_char = c(doc1,doc2,doc3,doc4,doc5)

doc_full = paste(doc_full_char, collapse=" ")

doc_table = tibble(doc_full)

doc_table$doc_full <- as.character(doc_table$doc_full)

View(doc_table)

```
### Word Frequencies
```{r}
ah_word <- doc_table %>%
  unnest_tokens(word, doc_full)
ah_count <- ah_word %>%
  count(word, sort=TRUE)

view(ah_count)
ah_count$word <- as.factor(ah_count$word)  

ah_word_sw <- ah_word %>%
      anti_join(stop_words)

ah_count_sw <- ah_word_sw %>%
  count(word, sort=TRUE)

ah_count_sw$word <- as.factor(ah_count_sw$word) 

ggplot(
  data = ah_count_sw[1:20,],
  aes(x = fct_reorder(word,n), 
      y = n)
  ) + 
  geom_col() + 
  coord_flip()+
  theme_light()



```

### Tokenization 
```{r}

#Tokenization by word

doc_word = doc_table %>% 
  unnest_tokens(word, doc_full)%>%
  anti_join(stop_words)%>%
  count(word,sort=TRUE)

View(doc_word)

#Tokenization by sentence
doc_sentence = doc_table %>% 
  unnest_tokens(sentence, doc_full, token = "sentences")
View(doc_sentence)

```


```{r}
#helps with the sentiment analysis, using package "textdata"

get_sentiments('afinn')# we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc')# looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing')# looks like a good amount more 6,776, but as we can see just negative and positive. 

virginia_sentiment_afinn = doc_word %>% inner_join(get_sentiments("afinn"))

virginia_sentiment_nrc = doc_word %>% inner_join(get_sentiments("nrc"))

virginia_sentiment_bing = doc_word %>% inner_join(get_sentiments("bing"))

View(virginia_sentiment_afinn)#discrete integer rating
View(virginia_sentiment_nrc)#a sentiment cateogry
View(virginia_sentiment_bing)#binary positive or negative classification

```

### Virginia Climate Change Sentiment Range
```{r}

ggplot(data = virginia_sentiment_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Virginia Climate Change Sentiment Range")+
  theme_minimal()


ggplot(data = virginia_sentiment_nrc, 
       aes(x=sentiment)
        )+
  geom_histogram(stat = "count")+
  ggtitle("Virginia Climate Change Sentiment Categories")+
  theme_minimal()

ggplot(data = virginia_sentiment_bing, 
       aes(x=sentiment)
        )+
  geom_histogram(stat = "count")+
  ggtitle("Virginia Climate Change Sentiment")+
  theme_minimal()
```

### Word Cloud

```{r}
set.seed(42)
ggplot(doc_word[1:100,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()
```

### Term frequency - inverse document 
```{r}
doc1 = read_lines("text_data/virginia/virginia_1.txt")
doc2 = read_lines("text_data/virginia/virginia_2.txt")
doc3 = read_lines("text_data/virginia/virginia_3.txt")
doc4 = read_lines("text_data/virginia/virginia_4.txt")
doc5 = read_lines("text_data/virginia/virginia_5.txt")


data_prep <- function(x){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",remove = TRUE,sep = "")
}

doc1_bag = data_prep(doc1)
doc2_bag = data_prep(doc2)
doc3_bag = data_prep(doc3)
doc4_bag = data_prep(doc4)
doc5_bag = data_prep(doc5)

View(doc2_bag)

docs = c("doc1", "doc2", "doc3", "doc4", "doc5")
tf_idf_text = tibble(docs, text=t(tibble(doc1_bag, doc2_bag, doc3_bag, doc4_bag, doc5_bag, .name_repair="universal")))

View(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(docs, word, sort = TRUE)


total_words <- word_count %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

virginia_words <- left_join(word_count, total_words)

View(virginia_words)

virginia_words <- virginia_words %>%
  bind_tf_idf(word, docs, n)

```

### Conclusion













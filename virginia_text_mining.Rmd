---
title: "virginia_text_mining"
output: html_document
---

To Do: word map, recommendations (final report), Words ordered by frequency

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(tidyverse)
install.packages("tidytext")
library(tidytext)
install.packages("ggwordcloud")
library(ggwordcloud)
install.packages("gutenbergr") 
library(gutenbergr)
install.packages('textdata')
library(textdata)
```
```{r}

doc1 = read_lines("text_data/virginia/virginia_1.txt")
doc2 = read_lines("text_data/virginia/virginia_2.txt")
doc3 = read_lines("text_data/virginia/virginia_3.txt")
doc4 = read_lines("text_data/virginia/virginia_4.txt")
doc5 = read_lines("text_data/virginia/virginia_5.txt")


doc_full_char = c(doc1,doc2,doc3,doc4,doc5)

doc_full = paste(doc_full_char, collapse=" ")

doc_table = tibble(doc_full)

doc_table$doc_full <- as.character(doc_table$doc_full)

View(doc_table)

```
### Word Frequencies
```{r}
ah_word <- doc_table %>%
  unnest_tokens(word, doc_full)
ah_count <- ah_word %>%
  count(word, sort=TRUE)

view(ah_count)
ah_count$word <- as.factor(ah_count$word)  

ah_word_sw <- ah_word %>%
      anti_join(stop_words)

ah_count_sw <- ah_word_sw %>%
  count(word, sort=TRUE)

ah_count_sw$word <- as.factor(ah_count_sw$word) 

ggplot(
  data = ah_count_sw[1:20,],
  aes(x = fct_reorder(word,n), 
      y = n)
  ) + 
  geom_col() + 
  coord_flip()+
  theme_light()

```
According to these frequencies, climate, change, carbon, tourism, temperatures, and emissions are most commonly mentioned in Virginia news articles. These are words that one would expect to see when reading climate change related news articles. These words suggest that Virginia news articles are focused on describing the problem of climate change and factors involved such as carbon, tourism, and emissions.  

### Tokenization 
```{r}

#Tokenization by word

doc_word = doc_table %>% 
  unnest_tokens(word, doc_full)%>%
  anti_join(stop_words)%>%
  count(word,sort=TRUE)

View(doc_word)

#Tokenization by sentence
doc_sentence = doc_table %>% 
  unnest_tokens(sentence, doc_full, token = "sentences")
View(doc_sentence)

```


```{r}
#helps with the sentiment analysis, using package "textdata"

get_sentiments('afinn')# we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc')# looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing')# looks like a good amount more 6,776, but as we can see just negative and positive. 

virginia_sentiment_afinn = doc_word %>% inner_join(get_sentiments("afinn"))

virginia_sentiment_nrc = doc_word %>% inner_join(get_sentiments("nrc"))

virginia_sentiment_bing = doc_word %>% inner_join(get_sentiments("bing"))

View(virginia_sentiment_afinn)#discrete integer rating
View(virginia_sentiment_nrc)#a sentiment cateogry
View(virginia_sentiment_bing)#binary positive or negative classification

```

### Virginia Climate Change Sentiment Range
```{r}

ggplot(data = virginia_sentiment_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("Virginia Climate Change Sentiment Range")+
  theme_minimal()


ggplot(data = virginia_sentiment_nrc, 
       aes(x=sentiment)
        )+
  geom_histogram(stat = "count")+
  ggtitle("Virginia Climate Change Sentiment Categories")+
  theme_minimal()

ggplot(data = virginia_sentiment_bing, 
       aes(x=sentiment)
        )+
  geom_histogram(stat = "count")+
  ggtitle("Virginia Climate Change Sentiment")+
  theme_minimal()
```
Overall, Virginia news articles has a negative sentiment toward climate change suggesting that Virginia residents see climate change as a real issue.
According to the sentiment categories graph, positive sentiment was most frequent in addition to trust. This may be due to the constructive nature of these news articles. They are often calls to action which are supported by facts. The most frequent negative sentiments were fear and anticipation, which is inline with how climate change activists express their concerns of climate change.   

### Word Cloud

```{r}
set.seed(42)
ggplot(doc_word[1:100,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()
```

### Term frequency - inverse document 
```{r}
doc1 = read_lines("text_data/virginia/virginia_1.txt")
doc2 = read_lines("text_data/virginia/virginia_2.txt")
doc3 = read_lines("text_data/virginia/virginia_3.txt")
doc4 = read_lines("text_data/virginia/virginia_4.txt")
doc5 = read_lines("text_data/virginia/virginia_5.txt")


data_prep <- function(x){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",remove = TRUE,sep = "")
}

doc1_bag = data_prep(doc1)
doc2_bag = data_prep(doc2)
doc3_bag = data_prep(doc3)
doc4_bag = data_prep(doc4)
doc5_bag = data_prep(doc5)

View(doc2_bag)

docs = c("doc1", "doc2", "doc3", "doc4", "doc5")
tf_idf_text = tibble(docs, text=t(tibble(doc1_bag, doc2_bag, doc3_bag, doc4_bag, doc5_bag, .name_repair="universal")))

View(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(docs, word, sort = TRUE)


total_words <- word_count %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

virginia_words <- left_join(word_count, total_words)

View(virginia_words)

virginia_words <- virginia_words %>%
  bind_tf_idf(word, docs, n)

```

### Recommendations

Based on my analysis, it seems that Virginians see climate change as something to anticipate and be fearful of. Overall, Virginians have a negative
sentiment toward climate change because they see it as an important issue. I would recommend pursuing ways to make a direct effort to support climate change in virginia. Based on our word map and sentiment analysis, Virginians clearly know the major factors contributing to our changing climate. What's important for them are clear steps toward action to slow down climate change.











